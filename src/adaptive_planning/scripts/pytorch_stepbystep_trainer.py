#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäº PyTorchStepByStep çš„å¯è§£é‡Šè®­ç»ƒæµç¨‹
æä¾›æ¸…æ™°çš„æ­¥éª¤åŒ–è®­ç»ƒè¿‡ç¨‹
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
import os
import json
from datetime import datetime
import sys

# æ·»åŠ PyTorchStepByStepè·¯å¾„
THIRD_PARTY_PATH = os.path.join(os.path.dirname(__file__), '../../third_party')
PYTORCH_STEP_PATH = os.path.join(THIRD_PARTY_PATH, 'PyTorchStepByStep')
sys.path.append(PYTORCH_STEP_PATH)

# å¯¼å…¥DRL-Navç½‘ç»œ
from drl_nav_network import DRLNavNetwork, DRLNavRewardCalculator

class StepByStepTrainer:
    """
    åŸºäºPyTorchStepByStepç†å¿µçš„æ­¥éª¤åŒ–è®­ç»ƒå™¨
    æä¾›æ¸…æ™°ã€å¯è°ƒè¯•çš„è®­ç»ƒè¿‡ç¨‹
    """
    def __init__(self, model, device='cpu'):
        self.device = device
        self.model = model.to(device)
        self.losses = []
        self.val_losses = []
        self.learning_rates = []
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.loss_fn = None
        
        # æ•°æ®
        self.train_loader = None
        self.val_loader = None
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_loss = float('inf')
        
        print("ğŸ“š StepByStepè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def set_loaders(self, train_loader, val_loader=None):
        """Step 1: è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        self.train_loader = train_loader
        self.val_loader = val_loader
        print(f"âœ… Step 1: æ•°æ®åŠ è½½å™¨è®¾ç½®å®Œæˆ")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        if val_loader:
            print(f"   éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
    
    def set_optimizer(self, optimizer_class=torch.optim.Adam, **kwargs):
        """Step 2: è®¾ç½®ä¼˜åŒ–å™¨"""
        default_kwargs = {'lr': 1e-3, 'weight_decay': 1e-5}
        default_kwargs.update(kwargs)
        
        self.optimizer = optimizer_class(self.model.parameters(), **default_kwargs)
        print(f"âœ… Step 2: ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ - {optimizer_class.__name__}")
        print(f"   å‚æ•°: {default_kwargs}")
        
        return self
    
    def set_lr_scheduler(self, scheduler_class=torch.optim.lr_scheduler.StepLR, **kwargs):
        """Step 3: è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.optimizer is None:
            raise ValueError("è¯·å…ˆè®¾ç½®ä¼˜åŒ–å™¨")
        
        default_kwargs = {'step_size': 50, 'gamma': 0.95}
        default_kwargs.update(kwargs)
        
        self.scheduler = scheduler_class(self.optimizer, **default_kwargs)
        print(f"âœ… Step 3: å­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®å®Œæˆ - {scheduler_class.__name__}")
        print(f"   å‚æ•°: {default_kwargs}")
        
        return self
    
    def set_loss_function(self, loss_fn=nn.MSELoss()):
        """Step 4: è®¾ç½®æŸå¤±å‡½æ•°"""
        self.loss_fn = loss_fn
        print(f"âœ… Step 4: æŸå¤±å‡½æ•°è®¾ç½®å®Œæˆ - {type(loss_fn).__name__}")
        
        return self
    
    def train_step(self):
        """Step 5: å•æ­¥è®­ç»ƒ"""
        if not all([self.train_loader, self.optimizer, self.loss_fn]):
            raise ValueError("è¯·å…ˆå®ŒæˆSteps 1-4çš„è®¾ç½®")
        
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        for batch_idx, (states, targets) in enumerate(self.train_loader):
            states = states.to(self.device)
            targets = targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            predictions = self.model(states)
            loss = self.loss_fn(predictions, targets)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # è¯¦ç»†æ—¥å¿—ï¼ˆæ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡ï¼‰
            if batch_idx % 50 == 0:
                print(f"   Batch {batch_idx}/{len(self.train_loader)}, "
                      f"Loss: {loss.item():.6f}, "
                      f"LR: {self.optimizer.param_groups[0]['lr']:.6f}")
        
        avg_loss = total_loss / num_batches
        self.losses.append(avg_loss)
        
        if self.scheduler:
            self.scheduler.step()
            self.learning_rates.append(self.optimizer.param_groups[0]['lr'])
        
        return avg_loss
    
    def validation_step(self):
        """Step 6: éªŒè¯æ­¥éª¤"""
        if self.val_loader is None:
            return None
        
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for states, targets in self.val_loader:
                states = states.to(self.device)
                targets = targets.to(self.device)
                
                predictions = self.model(states)
                loss = self.loss_fn(predictions, targets)
                
                total_loss += loss.item()
                num_batches += 1
        
        avg_val_loss = total_loss / num_batches
        self.val_losses.append(avg_val_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < self.best_loss:
            self.best_loss = avg_val_loss
            self.save_checkpoint('best_model.pth')
        
        return avg_val_loss
    
    def train(self, epochs):
        """Step 7: å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {epochs} ä¸ªepoch...")
        
        for epoch in range(epochs):
            self.epoch = epoch
            
            print(f"\nğŸ“ˆ Epoch {epoch+1}/{epochs}")
            print("-" * 50)
            
            # è®­ç»ƒæ­¥éª¤
            train_loss = self.train_step()
            
            # éªŒè¯æ­¥éª¤
            val_loss = self.validation_step()
            
            # æ‰“å°epochç»“æœ
            print(f"ğŸ”¸ Epoch {epoch+1} ç»“æœ:")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            if val_loss is not None:
                print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
            if self.scheduler:
                print(f"   å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(f'checkpoint_epoch_{epoch+1}.pth')
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.6f}")
        return self
    
    def plot_training_curves(self, save_path=None):
        """Step 8: å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('Training Progress', fontsize=16)
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.losses, label='Training Loss', color='blue')
        if self.val_losses:
            axes[0, 0].plot(self.val_losses, label='Validation Loss', color='red')
        axes[0, 0].set_title('Loss Curves')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        if self.learning_rates:
            axes[0, 1].plot(self.learning_rates, color='green')
            axes[0, 1].set_title('Learning Rate Schedule')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Learning Rate')
            axes[0, 1].grid(True)
        
        # æŸå¤±åˆ†å¸ƒ
        axes[1, 0].hist(self.losses, bins=20, alpha=0.7, color='blue', label='Training')
        if self.val_losses:
            axes[1, 0].hist(self.val_losses, bins=20, alpha=0.7, color='red', label='Validation')
        axes[1, 0].set_title('Loss Distribution')
        axes[1, 0].set_xlabel('Loss Value')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].legend()
        
        # è®­ç»ƒè¶‹åŠ¿
        if len(self.losses) > 10:
            window = min(10, len(self.losses) // 5)
            smoothed_train = np.convolve(self.losses, np.ones(window)/window, mode='valid')
            axes[1, 1].plot(smoothed_train, label=f'Smoothed Training (window={window})', color='blue')
            if self.val_losses and len(self.val_losses) > window:
                smoothed_val = np.convolve(self.val_losses, np.ones(window)/window, mode='valid')
                axes[1, 1].plot(smoothed_val, label=f'Smoothed Validation (window={window})', color='red')
            axes[1, 1].set_title('Smoothed Training Curves')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Smoothed Loss')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def save_checkpoint(self, filename):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'losses': self.losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'best_loss': self.best_loss
        }
        
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
    
    def load_checkpoint(self, filename):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filename, map_location=self.device)
        
        self.epoch = checkpoint['epoch']
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        if self.optimizer and checkpoint['optimizer_state_dict']:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.losses = checkpoint.get('losses', [])
        self.val_losses = checkpoint.get('val_losses', [])
        self.learning_rates = checkpoint.get('learning_rates', [])
        self.best_loss = checkpoint.get('best_loss', float('inf'))
        
        print(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {filename}")
        print(f"   å½“å‰epoch: {self.epoch}")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.6f}")

def create_synthetic_dataset(num_samples=10000):
    """åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæ¼”ç¤º"""
    print("ğŸ”§ åˆ›å»ºåˆæˆæ•°æ®é›†...")
    
    # ç”ŸæˆéšæœºçŠ¶æ€
    # çŠ¶æ€ï¼š[pos(3), goal(3), obs_density, avg_clear, speed, goal_dist, complexity, battery]
    states = []
    weights = []
    
    for _ in range(num_samples):
        # éšæœºä½ç½®å’Œç›®æ ‡
        pos = np.random.uniform(-10, 10, 3)
        goal = np.random.uniform(-10, 10, 3)
        
        # ç¯å¢ƒç‰¹å¾
        obs_density = np.random.uniform(0, 1)
        avg_clear = np.random.uniform(0.5, 3.0)
        speed = np.random.uniform(0, 2.0)
        goal_dist = np.linalg.norm(goal - pos)
        complexity = np.random.uniform(0, 1)
        battery = np.random.uniform(0.5, 1.0)
        
        state = np.concatenate([pos, goal, [obs_density, avg_clear, speed, goal_dist, complexity, battery]])
        
        # åŸºäºè§„åˆ™ç”Ÿæˆ"ä¸“å®¶"æƒé‡
        w_smooth = 1.0 + np.random.normal(0, 0.1)
        w_collision = 2.0 + obs_density * 5.0 + np.random.normal(0, 0.2)
        w_time = 0.3 + (1.0 / max(goal_dist, 0.1)) * 0.5 + np.random.normal(0, 0.05)
        corridor = 1.0 - obs_density * 0.4 + np.random.normal(0, 0.05)
        max_vel = 2.0 - obs_density * 0.8 + np.random.normal(0, 0.1)
        freq = 15.0 + complexity * 10.0 + np.random.normal(0, 1.0)
        
        weight = np.array([w_smooth, w_collision, w_time, corridor, max_vel, freq])
        
        states.append(state)
        weights.append(weight)
    
    # è½¬æ¢ä¸ºtensor
    states_tensor = torch.FloatTensor(np.array(states))
    weights_tensor = torch.FloatTensor(np.array(weights))
    
    print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ: {num_samples} æ ·æœ¬")
    print(f"   çŠ¶æ€ç»´åº¦: {states_tensor.shape[1]}")
    print(f"   æƒé‡ç»´åº¦: {weights_tensor.shape[1]}")
    
    return TensorDataset(states_tensor, weights_tensor)

def main_training_pipeline():
    """ä¸»è®­ç»ƒæµç¨‹ - PyTorchStepByStepé£æ ¼"""
    print("=" * 60)
    print("    ğŸ¯ è‡ªé€‚åº”æƒé‡å­¦ä¹  - PyTorchStepByStepè®­ç»ƒ")
    print("=" * 60)
    
    # è®¾å¤‡é€‰æ‹©
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # Step 0: å‡†å¤‡æ•°æ®
    print("\nğŸ“Š Step 0: å‡†å¤‡æ•°æ®...")
    dataset = create_synthetic_dataset(num_samples=10000)
    
    # æ•°æ®åˆ†å‰²
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    model = DRLNavNetwork(state_dim=12, action_dim=6)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = StepByStepTrainer(model, device)
    
    # Step by Step è®¾ç½®
    trainer.set_loaders(train_loader, val_loader)
    trainer.set_optimizer(torch.optim.Adam, lr=3e-4, weight_decay=1e-5)
    trainer.set_lr_scheduler(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9)
    trainer.set_loss_function(nn.MSELoss())
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(epochs=100)
    
    # å¯è§†åŒ–ç»“æœ
    trainer.plot_training_curves('training_curves.png')
    
    # å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
    model_export_path = 'adaptive_weights_stepbystep.ts'
    example_input = torch.randn(1, 12).to(device)
    traced_model = torch.jit.trace(model.eval(), example_input)
    traced_model.save(model_export_path)
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²å¯¼å‡º: {model_export_path}")
    
    return trainer

if __name__ == "__main__":
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs('models', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # è¿è¡Œè®­ç»ƒ
    trainer = main_training_pipeline()
