#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºŽ gym-pybullet-drones çš„è‡ªé€‚åº”æƒé‡å­¦ä¹ çŽ¯å¢ƒ
ç›´æŽ¥ä½¿ç”¨ gym-pybullet-drones çš„ä»£ç ï¼Œæ·»åŠ æƒé‡é¢„æµ‹ä»»åŠ¡
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import gymnasium as gym
from stable_baselines3 import PPO, SAC
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
import argparse

# æ·»åŠ ç¬¬ä¸‰æ–¹è·¯å¾„
THIRD_PARTY_PATH = os.path.join(os.path.dirname(__file__), '../../third_party')
sys.path.append(os.path.join(THIRD_PARTY_PATH, 'gym-pybullet-drones'))
sys.path.append(os.path.join(THIRD_PARTY_PATH, 'DRL-Nav'))

try:
    from gym_pybullet_drones.envs.HoverAviary import HoverAviary
    from gym_pybullet_drones.envs.FlyThruGateAviary import FlyThruGateAviary
    from gym_pybullet_drones.utils.enums import DroneModel, Physics
    print("âœ“ gym-pybullet-drones å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥ gym-pybullet-drones: {e}")
    print("è¯·å…ˆè¿è¡Œ: pip install gym-pybullet-drones")
    sys.exit(1)

class AdaptiveWeightsPredictionEnv(gym.Wrapper):
    """
    åŒ…è£… gym-pybullet-drones çŽ¯å¢ƒï¼Œç”¨äºŽå­¦ä¹ è‡ªé€‚åº”æƒé‡é¢„æµ‹
    """
    def __init__(self, base_env):
        super().__init__(base_env)
        
        # æ‰©å±•è§‚æµ‹ç©ºé—´ï¼šåŽŸå§‹obs + çŽ¯å¢ƒç‰¹å¾
        original_obs_dim = base_env.observation_space.shape[0]
        # æ·»åŠ çŽ¯å¢ƒç‰¹å¾ï¼šéšœç¢å¯†åº¦ã€è·ç¦»ç›®æ ‡ã€å¤æ‚åº¦ç­‰
        env_features_dim = 6  
        new_obs_dim = original_obs_dim + env_features_dim
        
        # ä¿®æ”¹è§‚æµ‹ç©ºé—´
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(new_obs_dim,), dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´ï¼šé¢„æµ‹6ä¸ªæƒé‡å‚æ•°
        # [w_smooth, w_collision, w_time, corridor_width, max_velocity, replan_freq]
        self.action_space = gym.spaces.Box(
            low=np.array([0.1, 1.0, 0.1, 0.3, 0.5, 5.0]),
            high=np.array([2.0, 10.0, 1.0, 1.5, 3.0, 30.0]),
            dtype=np.float32
        )
        
        # å†…éƒ¨çŠ¶æ€
        self.step_count = 0
        self.start_pos = None
        self.goal_pos = np.array([2.0, 0.0, 1.0])  # é»˜è®¤ç›®æ ‡
        
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.step_count = 0
        self.start_pos = obs[:3] if len(obs) >= 3 else np.zeros(3)
        
        # éšæœºåŒ–ç›®æ ‡ç‚¹
        self.goal_pos = np.random.uniform([-5, -5, 0.5], [5, 5, 2.0])
        
        # æ‰©å±•è§‚æµ‹
        extended_obs = self._extend_observation(obs)
        return extended_obs, info
        
    def step(self, action):
        # actionæ˜¯é¢„æµ‹çš„æƒé‡ï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦è½¬æ¢ä¸ºå®žé™…çš„æŽ§åˆ¶å‘½ä»¤
        # ç®€åŒ–å¤„ç†ï¼šæƒé‡å½±å“æŽ§åˆ¶ç­–ç•¥
        weights = action
        
        # åŸºäºŽæƒé‡ç”ŸæˆæŽ§åˆ¶æŒ‡ä»¤ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºæœå‘ç›®æ ‡çš„é€Ÿåº¦ï¼‰
        current_pos = self.get_current_position()
        direction = self.goal_pos - current_pos
        distance = np.linalg.norm(direction)
        
        if distance > 0.1:
            # æ ¹æ®æƒé‡è°ƒæ•´é€Ÿåº¦
            max_speed = weights[4] if len(weights) > 4 else 1.0  # max_velocity weight
            speed = min(max_speed, distance * 0.5)
            control_action = (direction / distance) * speed
            # è¡¥é½åˆ°çŽ¯å¢ƒéœ€è¦çš„åŠ¨ä½œç»´åº¦
            if hasattr(self.env, 'action_space'):
                action_dim = self.env.action_space.shape[0]
                if action_dim == 4:  # RPMæŽ§åˆ¶
                    control_action = np.array([0.5, 0.5, 0.5, 0.5]) + np.random.normal(0, 0.1, 4)
                    control_action = np.clip(control_action, 0, 1)
        else:
            control_action = np.zeros(self.env.action_space.shape[0])
            
        # æ‰§è¡ŒåŽŸå§‹çŽ¯å¢ƒæ­¥è¿›
        obs, reward, done, truncated, info = self.env.step(control_action)
        
        # é‡æ–°è®¡ç®—å¥–åŠ±ï¼ˆåŸºäºŽæƒé‡é¢„æµ‹è´¨é‡ï¼‰
        custom_reward = self._calculate_weight_prediction_reward(weights, obs)
        
        # æ‰©å±•è§‚æµ‹
        extended_obs = self._extend_observation(obs)
        
        self.step_count += 1
        return extended_obs, custom_reward, done, truncated, info
    
    def get_current_position(self):
        """èŽ·å–å½“å‰ä½ç½®ï¼ˆä»ŽçŽ¯å¢ƒçŠ¶æ€ï¼‰"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çŽ¯å¢ƒè°ƒæ•´
        try:
            if hasattr(self.env, '_getDroneStateVector'):
                state = self.env._getDroneStateVector(0)
                return state[:3]  # ä½ç½®
        except:
            pass
        return self.start_pos if self.start_pos is not None else np.zeros(3)
    
    def _extend_observation(self, obs):
        """æ‰©å±•è§‚æµ‹ç©ºé—´ï¼Œæ·»åŠ çŽ¯å¢ƒç‰¹å¾"""
        current_pos = obs[:3] if len(obs) >= 3 else np.zeros(3)
        
        # è®¡ç®—çŽ¯å¢ƒç‰¹å¾
        goal_distance = np.linalg.norm(self.goal_pos - current_pos)
        obstacle_density = self._estimate_obstacle_density(current_pos)
        path_complexity = min(1.0, goal_distance / 10.0 + obstacle_density)
        current_speed = np.linalg.norm(obs[10:13]) if len(obs) >= 13 else 0.0
        avg_clearance = max(0.5, 2.0 - obstacle_density)
        battery_level = max(0.5, 1.0 - self.step_count / 1000.0)  # ç®€åŒ–çš„ç”µé‡æ¨¡åž‹
        
        env_features = np.array([
            goal_distance, obstacle_density, path_complexity,
            current_speed, avg_clearance, battery_level
        ], dtype=np.float32)
        
        # æ‹¼æŽ¥åŽŸå§‹è§‚æµ‹å’ŒçŽ¯å¢ƒç‰¹å¾
        extended_obs = np.concatenate([obs, env_features])
        return extended_obs.astype(np.float32)
    
    def _estimate_obstacle_density(self, position):
        """ä¼°è®¡å½“å‰ä½ç½®çš„éšœç¢å¯†åº¦"""
        # ç®€åŒ–çš„éšœç¢å¯†åº¦ä¼°è®¡
        # å¯ä»¥åŸºäºŽçŽ¯å¢ƒçš„å®žé™…éšœç¢ä¿¡æ¯
        x, y, z = position
        density = 0.0
        
        # ç®€å•çš„åŸºäºŽä½ç½®çš„å¯†åº¦æ¨¡åž‹
        if abs(x) > 3 or abs(y) > 3:  # è¾¹ç•ŒåŒºåŸŸå¯†åº¦é«˜
            density += 0.3
        if x > 0 and y > 0:  # æŸä¸ªè±¡é™å¯†åº¦é«˜
            density += 0.4
            
        return min(1.0, density + np.random.normal(0, 0.1))
    
    def _calculate_weight_prediction_reward(self, weights, obs):
        """è®¡ç®—æƒé‡é¢„æµ‹çš„å¥–åŠ±"""
        current_pos = obs[:3] if len(obs) >= 3 else np.zeros(3)
        
        # åŸºç¡€å¥–åŠ±ï¼šåˆ°è¾¾ç›®æ ‡
        goal_distance = np.linalg.norm(self.goal_pos - current_pos)
        reach_reward = 10.0 if goal_distance < 0.3 else -goal_distance * 0.1
        
        # æƒé‡åˆç†æ€§å¥–åŠ±
        w_smooth, w_collision, w_time = weights[0], weights[1], weights[2]
        corridor_width, max_velocity = weights[3], weights[4]
        
        # æƒé‡åº”è¯¥é€‚åº”çŽ¯å¢ƒ
        obstacle_density = self._estimate_obstacle_density(current_pos)
        
        # éšœç¢å¯†é›†æ—¶ï¼Œé¿éšœæƒé‡åº”è¯¥é«˜
        collision_appropriateness = w_collision * obstacle_density
        # é€Ÿåº¦æƒé‡åº”è¯¥åˆç†
        speed_appropriateness = 1.0 - abs(max_velocity - (2.0 - obstacle_density))
        
        weight_reward = (collision_appropriateness + speed_appropriateness) * 0.1
        
        # å¹³æ»‘åº¦å¥–åŠ±ï¼ˆé¿å…æƒé‡å‰§çƒˆå˜åŒ–ï¼‰
        smoothness_reward = -np.var(weights) * 0.01
        
        total_reward = reach_reward + weight_reward + smoothness_reward
        return total_reward

def create_training_env(env_id="hover", gui=False):
    """åˆ›å»ºè®­ç»ƒçŽ¯å¢ƒ"""
    if env_id == "hover":
        base_env = HoverAviary(
            drone_model=DroneModel.CF2X,
            initial_xyzs=np.array([[0, 0, 1]]),
            initial_rpys=np.array([[0, 0, 0]]),
            physics=Physics.PYB_GND_DRAG_DW,
            neighbourhood_radius=10,
            freq=240,
            aggregate_phy_steps=1,
            gui=gui,
            record=False,
            obstacles=True,  # æ·»åŠ éšœç¢ç‰©
            user_debug_gui=False
        )
    else:
        # å¯ä»¥æ·»åŠ å…¶ä»–çŽ¯å¢ƒç±»åž‹
        base_env = HoverAviary(gui=gui)
    
    # åŒ…è£…æˆè‡ªé€‚åº”æƒé‡é¢„æµ‹çŽ¯å¢ƒ
    env = AdaptiveWeightsPredictionEnv(base_env)
    return env

def train_adaptive_weights_model():
    """è®­ç»ƒè‡ªé€‚åº”æƒé‡é¢„æµ‹æ¨¡åž‹"""
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", default="hover", help="Environment type")
    parser.add_argument("--algo", default="PPO", choices=["PPO", "SAC"])
    parser.add_argument("--steps", type=int, default=100000, help="Training steps")
    parser.add_argument("--gui", action="store_true", help="Show GUI")
    parser.add_argument("--output_dir", default="models", help="Output directory")
    args = parser.parse_args()
    
    print(f"ðŸš€ å¼€å§‹è®­ç»ƒè‡ªé€‚åº”æƒé‡æ¨¡åž‹...")
    print(f"   çŽ¯å¢ƒ: {args.env}")
    print(f"   ç®—æ³•: {args.algo}")
    print(f"   è®­ç»ƒæ­¥æ•°: {args.steps}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆ›å»ºçŽ¯å¢ƒ
    env = create_training_env(args.env, gui=args.gui)
    
    # åˆ›å»ºè¯„ä¼°çŽ¯å¢ƒ
    eval_env = create_training_env(args.env, gui=False)
    
    # é€‰æ‹©ç®—æ³•
    if args.algo == "PPO":
        model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            tensorboard_log=f"{args.output_dir}/tensorboard/"
        )
    else:  # SAC
        model = SAC(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            buffer_size=100000,
            batch_size=256,
            gamma=0.99,
            tau=0.02,
            tensorboard_log=f"{args.output_dir}/tensorboard/"
        )
    
    # è¯„ä¼°å›žè°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f"{args.output_dir}/best_model",
        log_path=f"{args.output_dir}/eval_logs",
        eval_freq=5000,
        deterministic=True,
        render=False
    )
    
    # å¼€å§‹è®­ç»ƒ
    model.learn(
        total_timesteps=args.steps,
        callback=eval_callback,
        progress_bar=True
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
    final_model_path = f"{args.output_dir}/adaptive_weights_final"
    model.save(final_model_path)
    
    # å¯¼å‡ºä¸ºTorchScriptï¼ˆç”¨äºŽROSéƒ¨ç½²ï¼‰
    try:
        export_torchscript(model, env.observation_space.shape[0], 
                          env.action_space.shape[0], 
                          f"{args.output_dir}/adaptive_weights.ts")
        print(f"âœ… æ¨¡åž‹å·²å¯¼å‡ºä¸º TorchScript: {args.output_dir}/adaptive_weights.ts")
    except Exception as e:
        print(f"âš ï¸ TorchScriptå¯¼å‡ºå¤±è´¥: {e}")
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡åž‹ä¿å­˜è‡³: {final_model_path}")
    return final_model_path

def export_torchscript(model, obs_dim, act_dim, output_path):
    """å¯¼å‡ºTorchScriptæ¨¡åž‹ç”¨äºŽROSéƒ¨ç½²"""
    class TorchScriptPolicy(torch.nn.Module):
        def __init__(self, sb3_model):
            super().__init__()
            # æå–SB3æ¨¡åž‹çš„ç½‘ç»œ
            if hasattr(sb3_model.policy, 'mlp_extractor'):
                # PPO
                self.features_extractor = sb3_model.policy.features_extractor
                self.mlp_extractor = sb3_model.policy.mlp_extractor
                self.action_net = sb3_model.policy.action_net
            else:
                # SAC
                self.actor = sb3_model.policy.actor
            
            self.obs_dim = obs_dim
            self.act_dim = act_dim
            
        def forward(self, obs):
            obs = obs.float()
            
            if hasattr(self, 'actor'):
                # SAC
                mean_actions = self.actor.mu(obs)
                return mean_actions
            else:
                # PPO  
                features = self.features_extractor(obs)
                latent_pi, _ = self.mlp_extractor(features)
                actions = self.action_net(latent_pi)
                return actions
    
    # åˆ›å»ºåŒ…è£…å™¨
    ts_policy = TorchScriptPolicy(model)
    ts_policy.eval()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_obs = torch.randn(1, obs_dim)
    
    # è¿½è¸ªå¹¶ä¿å­˜
    traced_model = torch.jit.trace(ts_policy, dummy_obs)
    traced_model.save(output_path)

if __name__ == "__main__":
    train_adaptive_weights_model()
