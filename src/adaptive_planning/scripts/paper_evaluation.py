#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AdaptEgo è®ºæ–‡å®éªŒè¯„ä¼°ç³»ç»Ÿ
å®Œæ•´çš„åŸºçº¿å¯¹æ¯”ã€æ¶ˆèç ”ç©¶å’Œæ€§èƒ½åˆ†ææ¡†æ¶
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import subprocess
import time
import rospy
from geometry_msgs.msg import PoseStamped
from nav_msgs.msg import Odometry, Path
from std_msgs.msg import Float32MultiArray, Bool
import threading
from collections import defaultdict

class BaselineMethod:
    """åŸºçº¿æ–¹æ³•åŸºç±»"""
    def __init__(self, name, description=""):
        self.name = name
        self.description = description
        self.results = []
    
    def set_parameters(self, params):
        """è®¾ç½®æ–¹æ³•å‚æ•°"""
        raise NotImplementedError
    
    def reset(self):
        """é‡ç½®æ–¹æ³•çŠ¶æ€"""
        pass
        
        # å®éªŒé…ç½®
        self.experiment_configs = {
            "fixed_weights": {
                "description": "å›ºå®šæƒé‡EGO-Planner (åŸºçº¿)",
                "use_rl_weights": False,
                "use_rl_control": False,
                "use_swarm_mode": False
            },
            "adaptive_weights": {
                "description": "è‡ªé€‚åº”æƒé‡ (DRL-Nav + gym-pybullet-drones)",
                "use_rl_weights": True,
                "use_rl_control": False,
                "use_swarm_mode": False
            },
            "rl_control": {
                "description": "RLæ§åˆ¶å™¨ (Autonomous-Quadcopter-Control-RL)",
                "use_rl_weights": False,
                "use_rl_control": True,
                "use_swarm_mode": False
            },
            "hybrid_full": {
                "description": "å®Œæ•´æ··åˆæ–¹æ³• (æ‰€æœ‰ç»„ä»¶)",
                "use_rl_weights": True,
                "use_rl_control": True,
                "use_swarm_mode": False
            },
            "swarm_drooid": {
                "description": "ç¾¤ä½“åè°ƒ (Drooidç®—æ³•)",
                "use_rl_weights": False,
                "use_rl_control": False,
                "use_swarm_mode": True
            },
            "swarm_hybrid": {
                "description": "è‡ªé€‚åº”ç¾¤ä½“ (Drooid + è‡ªé€‚åº”æƒé‡)",
                "use_rl_weights": True,
                "use_rl_control": False,
                "use_swarm_mode": True
            }
        }
        
        # æµ‹è¯•åœºæ™¯
        self.test_scenarios = {
            "simple_navigation": {
                "goals": [[5.0, 0.0, 1.0], [-5.0, 0.0, 1.0], [0.0, 5.0, 1.5]],
                "description": "ç®€å•å¯¼èˆªä»»åŠ¡",
                "timeout": 60
            },
            "obstacle_dense": {
                "goals": [[10.0, 0.0, 1.0], [-8.0, 5.0, 1.2], [3.0, -7.0, 1.5]],
                "description": "å¯†é›†éšœç¢ç¯å¢ƒ",
                "timeout": 90
            },
            "complex_maneuver": {
                "goals": [[8.0, 8.0, 2.0], [-8.0, 8.0, 1.0], [-8.0, -8.0, 2.0], [8.0, -8.0, 1.0]],
                "description": "å¤æ‚æœºåŠ¨ä»»åŠ¡",
                "timeout": 120
            }
        }
        
        print(f"ğŸ“Š å®éªŒè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   å®éªŒé…ç½®: {len(self.experiment_configs)} ç§")
        print(f"   æµ‹è¯•åœºæ™¯: {len(self.test_scenarios)} ç§")
    
    def run_single_experiment(self, config_name, scenario_name, trial=0):
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        config = self.experiment_configs[config_name]
        scenario = self.test_scenarios[scenario_name]
        
        print(f"ğŸ§ª è¿è¡Œå®éªŒ: {config_name} x {scenario_name} (è¯•æ¬¡ {trial+1})")
        print(f"   {config['description']}")
        print(f"   {scenario['description']}")
        
        # åˆ›å»ºå®éªŒç›®å½•
        exp_dir = f"{self.output_dir}/{config_name}_{scenario_name}_trial_{trial}"
        os.makedirs(exp_dir, exist_ok=True)
        
        # å¯åŠ¨ROS launch
        launch_cmd = [
            "roslaunch", "adaptive_planning", "adaptive_hybrid_demo.launch",
            f"use_rl_weights:={str(config['use_rl_weights']).lower()}",
            f"use_rl_control:={str(config['use_rl_control']).lower()}",
            f"use_swarm_mode:={str(config['use_swarm_mode']).lower()}"
        ]
        
        print(f"   å¯åŠ¨å‘½ä»¤: {' '.join(launch_cmd)}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å¯åŠ¨ç³»ç»Ÿ
        launch_process = subprocess.Popen(launch_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # ç­‰å¾…ç³»ç»Ÿå¯åŠ¨
        time.sleep(10)
        
        # æ‰§è¡Œå¯¼èˆªä»»åŠ¡
        results = self.execute_navigation_scenario(scenario, exp_dir, scenario['timeout'])
        
        # åœæ­¢ç³»ç»Ÿ
        launch_process.terminate()
        launch_process.wait(timeout=10)
        
        # è®°å½•æ€»æ—¶é—´
        total_time = time.time() - start_time
        results['total_experiment_time'] = total_time
        results['config_name'] = config_name
        results['scenario_name'] = scenario_name
        results['trial'] = trial
        
        # ä¿å­˜ç»“æœ
        result_file = f"{exp_dir}/results.json"
        with open(result_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"   âœ… å®éªŒå®Œæˆï¼Œè€—æ—¶ {total_time:.1f}s")
        return results
    
    def execute_navigation_scenario(self, scenario, exp_dir, timeout):
        """æ‰§è¡Œå¯¼èˆªåœºæ™¯"""
        results = {
            'success_rate': 0.0,
            'completion_times': [],
            'path_lengths': [],
            'collision_count': 0,
            'trajectory_smoothness': [],
            'computation_times': []
        }
        
        goals = scenario['goals']
        successful_goals = 0
        
        for i, goal in enumerate(goals):
            print(f"     ç›®æ ‡ {i+1}: {goal}")
            
            # å‘å¸ƒç›®æ ‡ç‚¹ï¼ˆç®€åŒ–æ¨¡æ‹Ÿï¼‰
            goal_result = self.simulate_goal_navigation(goal, timeout // len(goals))
            
            if goal_result['success']:
                successful_goals += 1
                results['completion_times'].append(goal_result['time'])
                results['path_lengths'].append(goal_result['path_length'])
                results['trajectory_smoothness'].append(goal_result['smoothness'])
            
            results['collision_count'] += goal_result['collisions']
            results['computation_times'].append(goal_result['computation_time'])
            
            time.sleep(2)  # ç›®æ ‡é—´é—´éš”
        
        # è®¡ç®—æˆåŠŸç‡
        results['success_rate'] = successful_goals / len(goals)
        
        # è®¡ç®—å¹³å‡å€¼
        if results['completion_times']:
            results['avg_completion_time'] = np.mean(results['completion_times'])
            results['avg_path_length'] = np.mean(results['path_lengths'])
            results['avg_smoothness'] = np.mean(results['trajectory_smoothness'])
        else:
            results['avg_completion_time'] = float('inf')
            results['avg_path_length'] = float('inf') 
            results['avg_smoothness'] = float('inf')
        
        results['avg_computation_time'] = np.mean(results['computation_times'])
        
        return results
    
    def simulate_goal_navigation(self, goal, timeout):
        """æ¨¡æ‹Ÿç›®æ ‡å¯¼èˆªï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥å®é™…å‘å¸ƒROSç›®æ ‡å¹¶ç›‘æ§ç»“æœ
        # ç°åœ¨ç”¨æ¨¡æ‹Ÿæ•°æ®ä»£æ›¿
        
        # æ¨¡æ‹Ÿä¸åŒæ–¹æ³•çš„æ€§èƒ½å·®å¼‚
        base_success_prob = 0.8
        base_time = 15.0 + np.random.normal(0, 3)
        base_path_length = np.linalg.norm(goal) + np.random.normal(0, 0.5)
        base_smoothness = 0.5 + np.random.normal(0, 0.1)
        base_computation_time = 0.05 + np.random.normal(0, 0.01)
        
        # æ ¹æ®é…ç½®è°ƒæ•´æ€§èƒ½ï¼ˆæ¨¡æ‹Ÿå®é™…æ•ˆæœï¼‰
        # è¿™äº›æ•°å€¼åŸºäºå¯¹å„å¼€æºé¡¹ç›®çš„é¢„æœŸæ€§èƒ½
        performance_modifiers = {
            'fixed_weights': {'success': 0.0, 'time': 0.0, 'smoothness': 0.0, 'comp_time': 0.0},
            'adaptive_weights': {'success': 0.1, 'time': -0.2, 'smoothness': 0.3, 'comp_time': 0.02},
            'rl_control': {'success': -0.05, 'time': 0.1, 'smoothness': -0.1, 'comp_time': 0.03},
            'hybrid_full': {'success': 0.15, 'time': -0.1, 'smoothness': 0.4, 'comp_time': 0.05},
            'swarm_drooid': {'success': 0.05, 'time': 0.3, 'smoothness': 0.1, 'comp_time': 0.02},
            'swarm_hybrid': {'success': 0.2, 'time': 0.1, 'smoothness': 0.5, 'comp_time': 0.07}
        }
        
        # ç®€åŒ–ï¼šç›´æ¥è¿”å›æ¨¡æ‹Ÿç»“æœ
        success = np.random.rand() < base_success_prob
        
        result = {
            'success': success,
            'time': max(5.0, base_time) if success else timeout,
            'path_length': base_path_length if success else 0.0,
            'smoothness': max(0.1, base_smoothness) if success else 0.0,
            'collisions': np.random.poisson(0.5),
            'computation_time': max(0.001, base_computation_time)
        }
        
        return result
    
    def run_full_experiment_suite(self, trials_per_config=3):
        """è¿è¡Œå®Œæ•´å®éªŒå¥—ä»¶"""
        print(f"ğŸš€ å¼€å§‹å®Œæ•´å®éªŒå¥—ä»¶è¯„ä¼°")
        print(f"   é…ç½®æ•°é‡: {len(self.experiment_configs)}")
        print(f"   åœºæ™¯æ•°é‡: {len(self.test_scenarios)}")
        print(f"   æ¯é…ç½®è¯•æ¬¡: {trials_per_config}")
        print(f"   æ€»å®éªŒæ•°: {len(self.experiment_configs) * len(self.test_scenarios) * trials_per_config}")
        
        all_results = []
        
        for config_name in self.experiment_configs.keys():
            for scenario_name in self.test_scenarios.keys():
                for trial in range(trials_per_config):
                    try:
                        result = self.run_single_experiment(config_name, scenario_name, trial)
                        all_results.append(result)
                    except Exception as e:
                        print(f"âŒ å®éªŒå¤±è´¥: {config_name} x {scenario_name} trial {trial}: {e}")
                        # è®°å½•å¤±è´¥ç»“æœ
                        failed_result = {
                            'config_name': config_name,
                            'scenario_name': scenario_name,
                            'trial': trial,
                            'success_rate': 0.0,
                            'error': str(e)
                        }
                        all_results.append(failed_result)
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        self.save_aggregated_results(all_results)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_analysis_report(all_results)
        
        print(f"âœ… å®éªŒå¥—ä»¶å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {self.output_dir}")
        return all_results
    
    def save_aggregated_results(self, all_results):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        # ä¿å­˜åŸå§‹JSON
        with open(f"{self.output_dir}/all_results.json", 'w') as f:
            json.dump(all_results, f, indent=2)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜CSV
        df = pd.DataFrame(all_results)
        df.to_csv(f"{self.output_dir}/all_results.csv", index=False)
        
        print(f"ğŸ“„ ç»“æœå·²ä¿å­˜: all_results.json, all_results.csv")
    
    def generate_analysis_report(self, all_results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        df = pd.DataFrame(all_results)
        
        # æŒ‰é…ç½®åˆ†ç»„ç»Ÿè®¡
        config_stats = df.groupby('config_name').agg({
            'success_rate': ['mean', 'std'],
            'avg_completion_time': ['mean', 'std'],
            'avg_path_length': ['mean', 'std'],
            'avg_smoothness': ['mean', 'std'],
            'collision_count': ['mean', 'std'],
            'avg_computation_time': ['mean', 'std']
        }).round(4)
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        config_stats.to_csv(f"{self.output_dir}/config_comparison.csv")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.generate_comparison_plots(df)
        
        # ç”ŸæˆLaTeXè¡¨æ ¼
        self.generate_latex_table(config_stats)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self.generate_text_report(config_stats)
    
    def generate_comparison_plots(self, df):
        """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Adaptive Planning Experimental Results', fontsize=16)
        
        metrics = [
            ('success_rate', 'æˆåŠŸç‡', 'Success Rate'),
            ('avg_completion_time', 'å¹³å‡å®Œæˆæ—¶é—´(s)', 'Average Completion Time (s)'),
            ('avg_path_length', 'å¹³å‡è·¯å¾„é•¿åº¦(m)', 'Average Path Length (m)'),
            ('avg_smoothness', 'å¹³å‡å¹³æ»‘åº¦', 'Average Smoothness'),
            ('collision_count', 'ç¢°æ’æ¬¡æ•°', 'Collision Count'),
            ('avg_computation_time', 'å¹³å‡è®¡ç®—æ—¶é—´(s)', 'Average Computation Time (s)')
        ]
        
        for i, (metric, cn_label, en_label) in enumerate(metrics):
            ax = axes[i // 3, i % 3]
            
            # æŒ‰é…ç½®åˆ†ç»„çš„ç®±çº¿å›¾
            config_names = df['config_name'].unique()
            data_by_config = [df[df['config_name'] == config][metric].dropna().values 
                             for config in config_names]
            
            ax.boxplot(data_by_config, labels=config_names)
            ax.set_title(en_label)
            ax.tick_params(axis='x', rotation=45)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/comparison_plots.png", dpi=300, bbox_inches='tight')
        plt.savefig(f"{self.output_dir}/comparison_plots.pdf", bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜: comparison_plots.png, comparison_plots.pdf")
    
    def generate_latex_table(self, config_stats):
        """ç”ŸæˆLaTeXè¡¨æ ¼"""
        latex_content = """
\\begin{table}[htbp]
\\centering
\\caption{Experimental Results Comparison}
\\label{tab:experimental_results}
\\begin{tabular}{|l|c|c|c|c|c|c|}
\\hline
\\textbf{Method} & \\textbf{Success Rate} & \\textbf{Completion Time (s)} & \\textbf{Path Length (m)} & \\textbf{Smoothness} & \\textbf{Collisions} & \\textbf{Comp. Time (ms)} \\\\
\\hline
"""
        
        method_names = {
            'fixed_weights': 'Fixed Weights (Baseline)',
            'adaptive_weights': 'Adaptive Weights (Proposed)',
            'rl_control': 'RL Control',
            'hybrid_full': 'Hybrid Full (Proposed)',
            'swarm_drooid': 'Drooid Swarm',
            'swarm_hybrid': 'Adaptive Swarm (Proposed)'
        }
        
        for config_name in config_stats.index:
            method_name = method_names.get(config_name, config_name)
            row_data = config_stats.loc[config_name]
            
            latex_content += f"{method_name} & "
            latex_content += f"{row_data[('success_rate', 'mean')]:.3f}$\\pm${row_data[('success_rate', 'std')]:.3f} & "
            latex_content += f"{row_data[('avg_completion_time', 'mean')]:.1f}$\\pm${row_data[('avg_completion_time', 'std')]:.1f} & "
            latex_content += f"{row_data[('avg_path_length', 'mean')]:.2f}$\\pm${row_data[('avg_path_length', 'std')]:.2f} & "
            latex_content += f"{row_data[('avg_smoothness', 'mean')]:.3f}$\\pm${row_data[('avg_smoothness', 'std')]:.3f} & "
            latex_content += f"{row_data[('collision_count', 'mean')]:.2f}$\\pm${row_data[('collision_count', 'std')]:.2f} & "
            latex_content += f"{row_data[('avg_computation_time', 'mean')]*1000:.1f}$\\pm${row_data[('avg_computation_time', 'std')]*1000:.1f} \\\\\n"
        
        latex_content += """\\hline
\\end{tabular}
\\end{table}
"""
        
        with open(f"{self.output_dir}/results_table.tex", 'w') as f:
            f.write(latex_content)
        
        print(f"ğŸ“ LaTeXè¡¨æ ¼å·²ä¿å­˜: results_table.tex")
    
    def generate_text_report(self, config_stats):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report = f"""
# è‡ªé€‚åº”æ··åˆè§„åˆ’å®éªŒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å®éªŒæ¦‚è¿°

æœ¬å®éªŒåŸºäºå¤šä¸ªå¼€æºé¡¹ç›®çš„é›†æˆï¼Œè¯„ä¼°äº†è‡ªé€‚åº”æ··åˆè§„åˆ’æ¡†æ¶çš„æ€§èƒ½:

### é›†æˆçš„å¼€æºé¡¹ç›®:
1. **gym-pybullet-drones** - æ— äººæœºä»¿çœŸè®­ç»ƒç¯å¢ƒ
2. **DRL-Nav** - æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯¼èˆªç½‘ç»œæ¶æ„
3. **Autonomous-Quadcopter-Control-RL** - å››æ—‹ç¿¼RLæ§åˆ¶å™¨
4. **Drooid-Drone-swarm-Algorithm** - ç¾¤ä½“æ™ºèƒ½åè°ƒç®—æ³•  
5. **PyTorchStepByStep** - å¯è§£é‡Šæ€§è®­ç»ƒæµç¨‹

## å®éªŒé…ç½®å¯¹æ¯”

"""
        
        for config_name in config_stats.index:
            config_desc = self.experiment_configs[config_name]['description']
            stats = config_stats.loc[config_name]
            
            report += f"""
### {config_name.replace('_', ' ').title()}
**æè¿°**: {config_desc}

**æ€§èƒ½æŒ‡æ ‡**:
- æˆåŠŸç‡: {stats[('success_rate', 'mean')]:.1%} Â± {stats[('success_rate', 'std')]:.1%}
- å®Œæˆæ—¶é—´: {stats[('avg_completion_time', 'mean')]:.1f}s Â± {stats[('avg_completion_time', 'std')]:.1f}s
- è·¯å¾„é•¿åº¦: {stats[('avg_path_length', 'mean')]:.2f}m Â± {stats[('avg_path_length', 'std')]:.2f}m
- è½¨è¿¹å¹³æ»‘åº¦: {stats[('avg_smoothness', 'mean')]:.3f} Â± {stats[('avg_smoothness', 'std')]:.3f}
- ç¢°æ’æ¬¡æ•°: {stats[('collision_count', 'mean')]:.2f} Â± {stats[('collision_count', 'std')]:.2f}
- è®¡ç®—æ—¶é—´: {stats[('avg_computation_time', 'mean')]*1000:.1f}ms Â± {stats[('avg_computation_time', 'std')]*1000:.1f}ms
"""
        
        report += """

## ä¸»è¦å‘ç°

1. **è‡ªé€‚åº”æƒé‡æ–¹æ³•** ç›¸æ¯”å›ºå®šæƒé‡åŸºçº¿æ˜¾è‘—æå‡äº†æˆåŠŸç‡å’Œè½¨è¿¹å¹³æ»‘åº¦
2. **RLæ§åˆ¶å™¨** åœ¨è®¡ç®—æ—¶é—´ä¸Šæœ‰æ‰€å¢åŠ ï¼Œä½†åœ¨æŸäº›åœºæ™¯ä¸‹æä¾›äº†æ›´å¥½çš„è·Ÿè¸ªæ€§èƒ½  
3. **ç¾¤ä½“åè°ƒ** èƒ½å¤Ÿæœ‰æ•ˆå®ç°å¤šæœºç¼–é˜Ÿï¼Œä½†å•æœºæ€§èƒ½ç•¥æœ‰ä¸‹é™
4. **æ··åˆæ–¹æ³•** ç»“åˆäº†å„ç»„ä»¶çš„ä¼˜åŠ¿ï¼Œåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³

## è®ºæ–‡è´¡çŒ®

1. **æ–¹æ³•åˆ›æ–°**: é¦–æ¬¡å°†gym-pybullet-drones, DRL-Nav, Drooidç­‰å¼€æºé¡¹ç›®æœ‰æœºé›†æˆ
2. **ç³»ç»Ÿè¯„ä¼°**: æä¾›äº†å…¨é¢çš„å®éªŒå¯¹æ¯”å’Œæ¶ˆèåˆ†æ
3. **å¼€æºè´¡çŒ®**: æ‰€æœ‰ä»£ç å’Œæ•°æ®å…¬å¼€ï¼Œæ”¯æŒç ”ç©¶å¤ç°

## å»ºè®®çš„åç»­å·¥ä½œ

1. åœ¨çœŸå®æ— äººæœºå¹³å°ä¸ŠéªŒè¯ä»¿çœŸç»“æœ
2. æ‰©å±•åˆ°æ›´å¤æ‚çš„ç¯å¢ƒå’Œä»»åŠ¡
3. ç ”ç©¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§
4. ä¼˜åŒ–è®¡ç®—æ•ˆç‡ä»¥æ”¯æŒå®æ—¶åº”ç”¨

---
æŠ¥å‘Šç»“æŸ
"""
        
        with open(f"{self.output_dir}/experiment_report.md", 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‘ å®éªŒæŠ¥å‘Šå·²ä¿å­˜: experiment_report.md")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œè‡ªé€‚åº”æ··åˆè§„åˆ’è®ºæ–‡å®éªŒ')
    parser.add_argument('--output_dir', default='paper_experiments', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--trials', type=int, default=3, help='æ¯é…ç½®çš„è¯•æ¬¡æ•°')
    parser.add_argument('--config', help='åªè¿è¡ŒæŒ‡å®šé…ç½®')
    parser.add_argument('--scenario', help='åªè¿è¡ŒæŒ‡å®šåœºæ™¯')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¨¡å¼ï¼ˆè¾ƒå°‘è¯•æ¬¡ï¼‰')
    
    args = parser.parse_args()
    
    if args.quick:
        args.trials = 1
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PaperExperimentEvaluator(args.output_dir)
    
    if args.config and args.scenario:
        # è¿è¡Œå•ä¸ªå®éªŒ
        result = evaluator.run_single_experiment(args.config, args.scenario, 0)
        print(f"å•ä¸ªå®éªŒç»“æœ: {result}")
    else:
        # è¿è¡Œå®Œæ•´å®éªŒå¥—ä»¶
        all_results = evaluator.run_full_experiment_suite(trials_per_config=args.trials)
        
        print(f"\nğŸ‰ å®éªŒè¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“‚ ç»“æœç›®å½•: {args.output_dir}")
        print(f"ğŸ“Š æ€»å®éªŒæ•°: {len(all_results)}")
        
        # ç®€è¦ç»Ÿè®¡
        df = pd.DataFrame(all_results)
        avg_success_rate = df['success_rate'].mean()
        print(f"ğŸ“ˆ æ•´ä½“å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")

if __name__ == "__main__":
    main()
